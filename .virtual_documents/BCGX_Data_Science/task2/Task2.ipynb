import pandas as pd
from pandas import option_context
import dask.dataframe as dd
import os








# !head -5 price_data.csv
!head -5 client_data.csv


!ls . -l -h


path = 'task2'
clients_path = os.path.join('','client_data.csv')
prices_path = os.path.join('','price_data.csv')








# each row corresponds to data about the unique customer
client = dd.read_csv(clients_path,sep=',', blocksize="30M")





client.dtypes





# show all rows in the dataframe
with option_context('display.max_columns', None):
    display(client.head(5))


# number of rows
client['id'].count().compute()


client[client['id'] == '038af19179925da21a25619c5a24b745'].compute()





prices = dd.read_csv(prices_path,sep=',',blocksize="30M")
prices.head()


prices['id'].count().compute()





# prices for each customer by month
prices[prices['id'] == '038af19179925da21a25619c5a24b745'].compute()





(client['churn'].value_counts(normalize=True)*100).compute()





client.describe().compute().round(3)





(client['churn'].value_counts(normalize=True).compute() * 100).round(2)





prices.describe().compute().round(3).T





churned = client[client['churn'] == 1]
notchurn = client[client['churn'] == 0]





churn_counts = churned.groupby(['channel_sales']).agg(counts=('id','count')).compute()
churn_counts['count%'] = ((churn_counts['counts']/churn_counts['counts'].sum()) * 100).round(2)
churn_counts = churn_counts.reset_index()
churn_counts.sort_values(by='count%',ascending=False)





def compare_percentage(column:str,df:pd.DataFrame=client):

    churn_counts = df.groupby([column,'churn']).agg(counts=('id','count')).compute()
    churn_counts = churn_counts.reset_index()
    all_counts = client.groupby([column]).agg(counts=('id','count')).compute()
    
    merged = churn_counts.merge(all_counts,left_on=column,right_index=True)
    merged['percent%'] = ((merged['counts_x']/merged['counts_y'])*100).round(2)
    merged = merged.groupby([column,'churn'])['percent%'].mean().to_frame().sort_index()
    return merged

compare_percentage('channel_sales')





gas_counts = churned.groupby(['has_gas']).agg(counts=('id','count')).compute()
gas_counts['count%'] = ((gas_counts['counts']/gas_counts['counts'].sum()) * 100).round(2)
gas_counts.sort_values(by='count%',ascending=False)


compare_percentage('has_gas')





import warnings; warnings.filterwarnings('ignore')

client.select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame()








client['nb_prod_act_cat'] = client['nb_prod_act'].astype('category')
compare_percentage('nb_prod_act_cat')





prices.head()





cols = ['price_off_peak_var','price_peak_var','price_mid_peak_var',
        'price_off_peak_fix','price_peak_fix','price_mid_peak_fix']

cols_diff = ['diff_price_off_peak_var','diff_price_peak_var','diff_price_mid_peak_var',
             'diff_price_off_peak_fix','diff_price_peak_fix','diff_price_mid_peak_fix']

cols_diff_cumsum = ['cdiff_price_off_peak_var','cdiff_price_peak_var','cdiff_price_mid_peak_var',
                    'cdiff_price_off_peak_fix','cdiff_price_peak_fix','cdiff_price_mid_peak_fix']

std_prices = prices.groupby('id')[cols].std().compute()


client_churn = client[['id','churn']].copy()


client_churn_stats = client_churn.merge(std_prices,left_on='id',right_index=True).compute()
client_churn_stats.index = client_churn_stats['id']
client_churn_stats = client_churn_stats.drop(['id'],axis=1)


client_churn_stats.corr().round(2).loc['churn'].to_frame()





for col_diff,col in zip(cols_diff,cols):
    prices[col_diff] = prices[col].diff()


# count the number of times there have been price hi
diff_price_off_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_var'] > 0).sum()).compute()
diff_price_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_var'] > 0).sum()).compute()
diff_price_mid_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_var'] > 0).sum()).compute()

diff_price_off_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_fix'] > 0).sum()).compute()
diff_price_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_fix'] > 0).sum()).compute()
diff_price_mid_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_fix'] > 0).sum()).compute()


diff_price_mid_peak_fix_counter


price_hikes = pd.concat([diff_price_off_peak_var_counter,diff_price_peak_var_counter,diff_price_mid_peak_var_counter,
                           diff_price_off_peak_fix_counter,diff_price_peak_fix_counter,diff_price_mid_peak_fix_counter],axis=1)
price_hikes.columns = cols_diff
price_hikes


client_churn = client[['id','churn']].copy()


client_churn_stats = client_churn.merge(price_hikes,left_on='id',right_index=True).compute()


client_churn_stats['sum'] = client_churn_stats[cols_diff].min(axis=1)
client_churn_stats.index = client_churn_stats['id']
client_churn_stats = client_churn_stats.drop(['id'],axis=1)


client_churn_stats.corr().round(2).loc['churn'].to_frame()





for cdiff,diff in zip(cols_diff_cumsum,cols_diff):
    prices[cdiff] = prices[diff].cumsum()


prices.columns





pdprices = prices.compute()
client_churn = client[['id','churn']].compute().copy()


pdprices


diff_price_off_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_var']].last()
diff_price_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_var']].last()
diff_price_mid_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_var']].last()

diff_price_off_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_fix']].last()
diff_price_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_fix']].last()
diff_price_mid_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_fix']].last()


price_accumulations = pd.concat([diff_price_off_peak_var_cumsum,diff_price_peak_var_cumsum,diff_price_mid_peak_var_cumsum,
                                 diff_price_off_peak_fix_cumsum,diff_price_peak_fix_cumsum,diff_price_mid_peak_fix_cumsum],axis=1)
price_accumulations
# price_accumulations.corr().round(2).loc['churn'].to_frame()


client_churn_stats = client_churn.merge(price_accumulations,left_on='id',right_index=True)
client_churn_stats = client_churn_stats.drop(['id'],axis=1)
client_churn_stats


client_churn_stats.corr().round(2).loc['churn'].to_frame()



